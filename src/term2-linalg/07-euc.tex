\begin{definition}
    Евклидовым пространством называется пара $(V, (\ ,\ ))$, так что $V$ --- векторное пространство над  $\R$.

    $(\ ,\ )\: V \times V \to \R$, такой что  $(\ ,\ )$ билинейна,  $(\ ,\ )$ --- симметрична и  $\forall v \in V\!: (v, v) \ge 0 \land (v, v) = 0 \iff v = 0$.

    Будем называть $(\ ,\ )$ скалярным произведением.
\end{definition}
\begin{example}
    $V=\R^n$. Тогда формула известна. Очев, что все очев.
\end{example}
\begin{example}
    $V = C[0, 1], (f, g) = \int\limits_0^{1}f(x)g(x) \mathrm{d}x$.
\end{example}
\begin{definition}
    $V$ --- евклидово  $v \in V$, норма  $V$ --- $\|v\| = \sqrt{(v, v)}$.

     $v_1, v_2 \in V, d(v_1, v_2) = \|v_1 - v_2\|$.
\end{definition}
\begin{statement}
    $d$ --- метрика.
\end{statement}
\begin{statement}
    $V$ --- евклидово. $v_1, v_2 \in V \implies |(v_1,v_2)| \le \|v_1\| \cdot \|v_2\|$.
\end{statement}
\begin{definition}
    $V$ --- евклидово. Тогда  $\langle v_1, v_2 \rangle$ --- это $\alpha \in [0; \pi]$,  такой, что  $\cos \alpha = \frac{(v_1, v_2)}{\|v_1\| \cdot \|v_2\|}$. 
\end{definition}
\begin{definition}
    Если $(v_1, v_2) = 0$, то будем называть $v_1, v_2$ ортогональными.
\end{definition}
\begin{definition}
    $V$ --- евклидово пространство,  $v_1, \ldots, v_n$ --- базис. Тогда матрица $a_{ij} = (v_i, v_j)$ --- матрица Грама.
\end{definition}

Возьмем $u_1, u_2 \in V$, с координатами $x_i, y_i$ в базисе  $\{v_i\}$. Тогда скалярное  $(u_1, u_2) = X^TGY$, где $G$ --- матрица Грама.

Если матрица Грама равна  $E$, то у нас ортонормированный базис и $X^TGY = X^TY=x_1y_1 + x_2y_2 + \ldots + x_ny_n$.

\begin{definition}
    Пусть $v_1, \ldots, v_n$ --- базис. Тогда он ортонормирован, если $\forall i,j\!: (v_i, v_j) = \delta_{ij}$.
\end{definition}
\begin{theorem}[оротогонализация Грама Шмидта]
    $V$ --- евклидово пространство,  $v_1, v_2, \ldots, v_n$ --- базис.
    Тогда $\exists$ ортонормированный базис (ОНБ)  $e_1, \ldots, e_n$, причем $\forall i\!: \langle v_1, \ldots, v_i\rangle \overset{(*)}{=} \langle e_1, \ldots, e_i\rangle$.
\end{theorem}
\begin{proof}
    Докажем, что $\exists$ базис со  $(*)$ по индукции.

    База: $e_1 = \frac{v_1}{\|v_1\|}$. $\langle e_1\rangle = \langle v_1\rangle$, $(e_1, e_1) = \frac{1}{\|v_1\|^2} (v_1, v_1) = 1$.

    Переход $l \to l+1$.  Строим  $\widetilde{e_{l+1}} = a_1e_1 + a_2e_2 + \ldots + v_{l+1}, a_i \in \R$. $\langle e_1, \ldots, e_l, \widetilde{e_{l+1}} \rangle = \langle e_1, e_2, \ldots, e_l, v_{l+1} \rangle = \langle v_1, v_2., \ldots, v_{l+1}\rangle$.

    Проверим ортогональность. Хочется: $\forall i\!: (a_1e_1 + \ldots + a_le_l + v_{l+1}, e_i) = 0$i

    Это следует из того, что можно раскрыть сумму $\sum\limits_{j=1}^l a_j(e_j, e_i) + (v_{l+1}, e_i) = 0 \overset{(*)}{\implies} a_i(e_i, e_i) + (v_{l+1}, e_i) = 0$. Тут почти все равно нулю, кроме  $(e_i, e_i) = 1$. Тогда  $a_i = - \frac{(v_{l+1}, e_i)}{(e_i, e_i)} = -(v_{l+1}, e_i)$. $(*)$ --- у нас по индукционному предположению получается, что  $(e_i, e_j) = \delta_{ij}$, поэтому почти все сократится.
\end{proof}
\begin{remark}
    Пусть $e_1, e_2, \ldots, e_n$ --- ОНБ,$v \in V$.  $x_1, \ldots, x_n$ --- координаты $v$. Тогда  $v\cdot e_i = (\sum x_i e_i, e_i) = \sum x_j(e_j, e_i) = x_i$.
\end{remark}

С предыдущей лекции нам известно, что скалярное произведение --- симметрическая билинейная форма над  $\R$.
\begin{example}
    $\R^2, f(\begin{pmatrix} x_1 \\ y_1 \end{pmatrix}, \begin{pmatrix} y_1 \\ y_2 \end{pmatrix}) =x_1y_1 - x_2y_2$ или $(x_1-x_2)(y_1-y_2)$ или $x_1y_2 - x_2y_1$.

    Первые две --- симметрическая билинейная форма, последняя симметрическая кососимметрическая. 
 
    Или $V=2^M$ и  $f(x,y) = | X \cap Y| \pmod 2$, если явно записывать, то  $f(\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}, \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}) = x_1y_1 + \ldots + x_ny_n$.
\end{example}
\begin{definition}
    Симметрическая билинейная форма на $V$ (над $K$) называется невырожденной если $(\forall y \in V: f(x, y) = 0) \implies x = 0$.
\end{definition}
\begin{definition}
    $V$ --- векторное пространство, то  $V^* = Hom(V, K)$ --- множество линейных отображений (функционалов $V \to K$).
\end{definition}

Пусть $f$ --- билинейная форма  $\implies \exists F\!: V \to V^*, x \mapsto f_x\!: f_x(g) = f(x, y)$.

$f$ --- невырождена $\iff F$ --- изоморфизм. 

Пусть $f$ --- билинейная форма  $\leadsto \Gamma_f = (a_{ij})$,  $a_{ij} = f(e_i, e_j)$, где  $\{e_i\}$ --- базис. Тогда  $f(\sum x_i e_i, \sum y_i e_i) = \sum a_{ij} x_i y_{hj} = X^T \Gamma_f Y$.

Перейдем теперь к $K = \CC$.  $f(\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}, \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}) = \sum x_i \overline{y}_i$, $f(x, x) > 0$.

\begin{definition}
    Унитарным пространством называется пара $(V, (-,-))$, где  $V$ --- векторное пространство над  $\CC$,  $(-, -)\!: V \times V \to \CC$, такое, что 
     \begin{enumerate}
     \item $(x_1 + x_2, y) = (x_1, y) + (x_2, y)$ или $(x, y_1 + y_2) = (x, y_1) + (x, y_2)$.
     \item $(ax, y) = a(x, y), (x, ay) = \overline{a}(x, y)$.
     \item $f(x, y) = \overline{f(y, x)}$.
     \item  $f(x, x) \in \R_+$, при  $x \neq 0$.
    \end{enumerate}
    Длина, КБШ, метрика --- все тоже самое.
\\
    С углами сложнее (это останется тайной).
    \\
    Отрогонализация Грама-Шмидта работает, в частности, $\forall$ унитарном пространстве есть ортонормированный базис. При этом $f(X, Y) = X^T\Gamma_f\overline{Y}$.
\end{definition}
\Subsection{Ортогональное дополнение}
 \begin{definition}
     $V$ --- эвклидово/унитарное пространство:  $U \le V$. Ортогональное дополнение $U$ --- это $U^\perp = \{ v \in V \mid (u,v) = 0 \forall u \in U\}$.
 \end{definition}
 \begin{statement}
     $U^\perp$ --- подпространство в  $V$.
 \end{statement}
 \begin{proof}
     Упражнение.
 \end{proof}
 \begin{theorem}
     \slashn
      \begin{enumerate}
          \item $\dim U^\perp = \dim V - \dim U$.
          \item  $U^{\perp\perp} = U$.
          \item  $V = U \oplus U^\perp$ --- любой элемент из $V$ представим в виде пары из  $U$ и  $U^\perp$.
     \end{enumerate}
 \end{theorem}
 \begin{proof}
     \slashn
     \begin{enumerate}
         \item $\dim V = n, \dim U = K$. Возьмем базис $e_1, \ldots, e_k$, дополним через $e_{k+1}, \ldots, e_n$ до ОНБ (до любого, а далее Грам-Шмидт).

             Заметим, что $x = \sum x_ie_i \in U^{\perp} \iff (\sum\limits_{i=1}^n x_ie_i, e_j) = 0 \forall j=1,..,k \xLeftrightarrow{(\sum x_i e_i, e_j) = x_j} x_j = 0 \forall j=1..k \iff x = \sum\limits_{k+1}^n x_ie_i \iff x \in \langle e_{k+1}, \ldots, e_n \rangle = U$.

            Итого, $\dim U^\perp = n - k = \dim V - \dim U$.

            Тогда  $U^{\perp\perp} = \langle e_{k+1}, \ldots, e_n \rangle ^{\perp} = \langle e_1, \ldots, e_k \rangle = U$. 

            Ну и понятно, как раскладывается вектор из $V$.
     \end{enumerate}
 \end{proof}
 \begin{definition}
     $v = u+u^{\perp}, u \in U, u^{\perp} \in U^{\perp}$,  $u$ --- проекция  $v$ на  $U$, $u^\perp$ --- ортогональная составляющая.

     $\|u^{\perp}\| = \min\limits_{\widetilde{u} \in U} \|v - \widetilde{u}\|$ --- расстояние от $v$ до  $u$.
 \end{definition}
\begin{remark}
     Для любых билинейных форм верно $U \subset U^{\perp\perp}$. Для невырожденных верно $1, 2$,  $3$ --- неверно.
 \end{remark}
 \begin{statement}[Матрица Грама при замене базиса]
     $f$ --- билинейная полуторномерная форма.  $\Gamma_f$ --- матрица Грама в базисе  $e_1, \ldots, e_n$.

     Возьмем другой базис, $C$ --- матрица перехода.

     $X \leadsto C X = \widetilde{X}$. Пусть  $D = C^{-1}$. Тогда  $X = D\widetilde{X}$.

     $f(x, y) = X^T\Gamma Y = (\widetilde{X}D)^T\Gamma (D\widetilde{Y})= \widetilde{X}^{T} (D^T\Gamma D)\widetilde{Y} \implies D^T\Gamma D = \widetilde{\Gamma}$ --- матрица Грама в новом базисе.
 \end{statement}
 \Subsection{Соответствиемежду формами/матрицами}

 Билинейная полуторная форма $\implies A \in M_n(K)$.

 Симметрическая билинейная форма  $\iff A = A^T$ --- симм. матрица. ($A = \Gamma_f$).

 Полуторная форма $\iff a_{ij} = \overline{a_{ji}} \iff \overline{A^T} = A$,  $A^* \coloneqq \overline{A^T}$ матричное сопряжение.

  $A = A^*$ --- эрмитова матрица.

 $f$ --- симметрическая билинейная/полуторная форма.

 Переформулировка: дана  $q(X) = f(X, X) = \sum a_{ij} x_i x_j$ --- квадратичная форма.

 Как понять: верно ли, что  $q(x) > 0 \ \forall x \neq 0$.

  $f$ -- положительно определена  $\implies f$ --- скалярное произведение  $\implies \exists $ ОНБ:  $\exists C\!: C^TAC = E$. Тогда $\det C^TAC = \det(C^T)\det(C)\det(A) = (\det C)^2 \det A = 1 = \det E \implies \det A > 0$

  \begin{theorem}[Критерий Сильвестра]
    $f$ --- симметричная билинейная,  $A \in M_n(\R)$ --- матрица Грама  $f$  в базисе $e_1, \ldots, e_n$.

    Тогда $f$ --- положительно определена  $\iff \forall i = 1..n \vartriangle_i > 0$, где $\vartriangle_i \coloneqq \det(a_{jk})_{\substack{j=1..i\\k=1..i}}$.
\end{theorem}
\begin{proof}
    Необходимость. $I = \{i_1, i_2, \ldots, i_k\}$ --- номера строк/столбцов.
\\
    $A_I$ --- подматрица со строками/столбцами из  $I$.  $A_I$ --- матрица Грама.  $f\Big|_{\langle e_{i_1}, \ldots, e_{i_k} \rangle}$ --- положительно определена.

    Достаточность. Пусть $\Delta_i > 0$. Докажем индукцией по $k$:  $f\Big|_{\langle e_1, \ldots, e_k\rangle}$ ---  положительно определена $\implies$ при  $k = n$ то , что надо.

    База:  $f \Big|_{\langle e_1 \rangle}\ f(ae_1, ae_1) = a^2(e_1, e_1) = a^2a_{11} > 0$.

    Переход: $k \to k + 1$.  $f\Big|_{\langle e_1, \ldots, e_k \rangle}$ --- положительно определена, $(\langle e_1, \ldots, e_k \rangle, f)$ --- евклидово пространство. $\implies \exists$ ОНБ, $\widetilde{e_1}, \ldots, \widetilde{e_k}$. Матрица Грама $f \Big|_{\langle e_1, \ldots, e_{k+1} \rangle}$ в базисе $\widetilde{e_1}, \ldots, \widetilde{e_k}, e_{k+1} =
    \left(\begin{array}{ccc|c}
    1 & \ldots & 0 & a_1\\
    \vdots & \ddots & \vdots & \vdots\\
    0 & \ldots & 1 & a_k\\ \hline
    a_1 & \ldots & a_k & a
\end{array}\right)$. 
$\widetilde{e}_{k+1} = e_{k+1} - \sum\limits_{i=1}^k a_i\widetilde{e_i}$. Теперь $(\widetilde{e}_{k+1}, \widetilde{e}_i) = (e_{k+1} - \sum a_i \widetilde{e_i}, \widetilde{e}_i) = (e_{k+1}, \widetilde{e}_i) - \sum\limits_{j=1}^k a_j(\widetilde{e_j}, \widetilde{e_i}) = a_i - a_i = 0$.

Тогда $\widetilde{A} = 
\begin{pmatrix}
    1 & \ldots & 0\\
    \vdots & \ddots & \vdots\\
    0 & \ldots & b
\end{pmatrix}$, $\det \widetilde{A} = \det (C^T) \cdot \Delta_{k+1} \det C > 0 \implies b > 0 \implies f$ имеет ОНБ в $\langle e_1, \ldots, e_{k+1} \rangle \implies f$ --- положительно определена на $\langle e_1, \ldots, e_{k+1} \rangle$.\\
*. $f(\sum c_i \widetilde{e_i}, \sum c_i \widetilde{e_i}) = \sum_{i=1}^k c_i^2 + c_{k+1}^{2}b > 0$
\end{proof}
\Section{Операторы в евклидовых и унитарных пространствах}{ХБ}
\Subsection{Самосопряженные операторы}
\begin{definition}
    $V$ --- евклидово/унитарное пространство,  $\CA \in End(V)$.

    $\mathcal{B}$ --- сопряженный оператор, если  $(\CA x, y) = (x, \mathcal{B}y) \forall x, y \in V$.
\end{definition}
\begin{theorem}
    $\exists \land \exists!$ сопряженный оператор $\CA^*$. $\CA = \CA^*$ --- называется самосопряженным.
\end{theorem}
\begin{theorem}
    $\CA$ --- самосопряжен  $\iff$ у  $\CA$ есть ОНБ из собственных векторов и все с.ч вещественны. 

    $\CA$ --- самосопряжен  $\iff$  $[\CA]_{\text{ОНБ}} = A$ такова, что $A^T = \overline{A}$,  $A = (\delta_{ij})$,  $\overline{A} = (\overline{\delta_{ij}})$.
\end{theorem}
\begin{proof}
    $\CA$ --- самосопряжен  $\iff$ $\forall i, j\!: (\CA e_i, e_j) = (e_i, \CA e_j) \iff (\sum\limits_{k=1}^n a_{ik} e_k, e_j) = (e_i, \sum\limits_{k=1}^n a_{jk}e_k) \iff (a_{ij}e_j, e_j) = (e_i, a_{ji}e_i) \iff a_{ij}(e_j, e_i) = \overline{a_{ji}} (e_i, e_i) \iff a_{ij} = \overline{a_{ji}}$.

    $\CA$ --- самосопряжена  $\implies \forall \lambda$ --- собственное число  $\lambda \in \R$, потому что, если взять собственный вектор $x, \CA x=\lambda x$, получим, что $(\CA x, x) = (x, \CA x) \implies (\lambda x, x) = (x, \lambda x) \implies \lambda (x, x) = \overline{\lambda} (x, x) \implies \lambda = \overline{\lambda}$

    В том числе для  $A \in M_n(\R), A = A^T$ --- все корни  $\chi_A(t)$ --- вещественны.
\end{proof}

\begin{statement}
    $\CA$ --- самосопряженный оператор,  $\CA \in End(V)$,  $U \le V$ --- инвариантное подпространство относительно $\CA$, тогда  $U^\perp$ --- инвариантное подпространство. 
\end{statement}
\begin{proof}
    $v \in U^\perp$, то есть  $(u, v) = 0\ \forall u \in U$,  $\CA v\!:\ (u, \CA V) = (\CA u, v) = 0 \implies \CA v \in u^\perp$.
\end{proof}
\begin{proof}[Доказтельство теоремы\dots]
    $\Leftarrow$. Пусть у  $\CA \exists$ ОНБ из собственных векторов с вещественными собственными числами. То есть в некотором ОНБ $[\CA]$ диагонализуема и  $\lambda_i \in \R$.  $A^T = \overline{A}(= A)$, потому что она диагональная и состоит из действительных чисел. 

    $\Rightarrow$. База $n=1$.

    Переход  $n \to n+1$.  $\CA$ --- самосопряженная,  $v_1, \ldots, v_{n+1}$ --- ОНБ. $[\CA]_{\{v_i\}} = A, A^T = \overline{A}, \exists \lambda_1 \in \R$ --- собственное число $A$.

    $\exists v \in V\!: \CA v = \lambda_1 v$, $\langle v \rangle$ --- инвариантное подпространство  $\implies \langle v \rangle^\perp$ --- инвариантное подпространство,  $\dim \langle v \rangle^{\perp} = n  + 1 - 1 = n$.  $\CA \Big|_{\langle v \rangle^{\perp}}\!: \langle v \rangle^\perp \to \langle v \rangle ^\perp$.

    $\CA\Big|_{\langle v \rangle^\perp}$ --- самосопряженнная по предположению.  $\exists e_1, \ldots ,e_n$ -- ОНБ из собственных векторов $\CA$ с вещественными собственными числами.

    $e_{n+1} \coloneqq \frac{v}{\|v\|}, \|e_{n+1}\| = 1$ и $\langle e_{n+1}, e_i \rangle = 0 \implies e_1, \ldots, e_{n+1}$ --- базис, который нам подходит. 
\end{proof}
\Subsection{Оценка квадратичной формы}

Хотим посмотреть на $q(x_1, \ldots, x_n) = \sum\limits_{i,j=1} a_{ij} x_ix_j$.

Ищем $A\!: q(x_1, \ldots, x_n) \le A(x_1^2 + \ldots + x_n^2)$, причем $A$ --- минимально.

Заметим, что  $\sum a_{ij} x_ix_j = (AX, X)$,  $X = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}, A = (a_{ij})$.

$X = \sum c_iv_i \implies(AX, X) = (\sum c_i \lambda_i v_i, \sum c_i v_i) = \sum \lambda_i c_i^2$.

Возьмем  $A = \max \{\lambda_i, 0\} \implies \sum \lambda_i c_i \le A \sum c_i^2$. Следовательно, $q(x_1, \ldots ,x_n) \le \lambda_{\max} (\sum x_i^2), \lambda_{max}$ --- максимальное собственное число.  Аналогично, $\ge \lambda_{min}(\sum x_i^2)$.
\Subsection{Ортогональные и унитарные операторы}
\begin{definition}
    $\CA \in End(V)$,  $V$ --- евклидово/унитарно.

    Тогда  $\CA$ называется ортогональным/унитарным оператором, если выполняется одно из равносильных свойств:
     \begin{enumerate}
         \item $(\CA X, \CA Y) = (x, y)\ \forall x, y \in V$,
         \item  $\|\CA x\| = \|X\|, \forall X \in V$,
         \item  $\CA$ --- обратим и  $(\CA X, Y) = (X, \CA^{-1}Y)$,
         \item В $\forall$ ОНБ  $\overline{A}^T = A^{-1}$,
         \item ОНБ  $\leadsto$ ОНБ  $\forall$ ОНБ,
         \item  $\exists$ ОНБ, такой что  $\leadsto$ ОНБ.
    \end{enumerate}
\end{definition}
\begin{proof}
    $1 \implies 2, 3 (x = y)$.

    $4 \iff 3$: очевидно как факт 1 про самосопряжение.
     $1 \implies 5, 6$ --- очев  $(x, y) \implies(\CA x, \CA y)$ 
     $6 \implies 1\!: \CA e_i = f_i, \{e_i\}, \{f_i\}$ --- ОНБ,  $x = \sum x_i e_i, y = \sum y_i f_i$.  \dots
\end{proof}
\begin{remark}
    $\CA$ --- самосопряженный  $\implies$ есть ОН собственный базис ($\lambda \in \R$). 

    $A = \overline{A}^T$ --- матрица  $\CA$ в ОНБ.  $\exists C\!: C^{-1}AC = C^{T}AC$. RHS сохраняется при применении  $\CA \implies$ LHS --- тоже.

    А значит любую квадратичную форму можно привести к виду  $\sum \lambda_i x_i^2$.
\end{remark}
\begin{property}
    $\CA$ --- ортогональный/унитарный оператор. $\lambda$ --- собственное число,  $(A \in M_n(\CC) \implies \overline{A}^T = A^{-1})$. 
    
    $\lambda$ --- собственное число $\implies |\lambda| = 1$,  $Av = \lambda v \implies(Av, v) = (v, A^{-1}v) \iff \lambda(v, v) = (v, \frac{1}{\lambda}v) = \frac{1}{\overline{\lambda}}(v, v) \implies \lambda \overline{\lambda} = 1$.\\
    В частности $|\det A| = 1$
\end{property}
\begin{definition}
Собственный ортогональный оператор — такой, что $\det A = 1$ (сохр-й ориентацию).
\end{definition}
\begin{remark}
    Ортогональные/унитарные операторы --- группа по умножению. Группы $O_n, SO_n, Un_n, SO_n$.
\end{remark}
\begin{example}
    $O_1 = \{\pm 1\}$, $SO_1 = \{1\}$, $U_1$ --- группа углов.
    
    $SU_1 = \{1\}$,  $SO_2$ --- группа углов.
\end{example}

\begin{theorem}
    $V$ --- унитарное пространство,  $\CA \in End(V)$ --- унитарный оператор  $\iff \exists$ ОНБ из собственных векторов  собственными числами  $\{\lambda_i\}, |\lambda_i| = 1 \quad \forall i$.
\end{theorem}
\begin{lemma}
    $U \le V$ --- инвариантное $\implies U^{\perp}$ --- инвариантное.
\end{lemma}
\begin{proof}
    $v \in u^\perp$ рассмотрим  $\CA v$:  $\forall u \in U\!: (u, \CA v) = (\CA^{-1}u, \CA^{-1}\CA v) = (\CA^{-1}u, v) = 0$.
\end{proof}
\begin{proof}
    см евклидов случай.
\end{proof}
\begin{theorem}
    $V$ --- евклидово пространство.  $\CA \in End(V)$ --- ортогонально  $\iff \exists$ ОНБ $e_1, \ldots, e_n$ такой что, $A$ --- блочная матрица из блоков ($n=2$)
    $\left(
        \begin{matrix}
            \cos \alpha & -\sin \alpha\\
            +\sin \alpha & \cos \alpha
        \end{matrix} \right)$
и возможно блоков размера $1$ из $\{-1, 1\}$.\\
То есть $\forall$ ортогональное преобразование — это композиция поворотов в нескольких попарно ортогональных плоскостях и зеркальных симметрий.
\end{theorem}
\begin{proof}
    $\CA \leadsto A$,  $A^T = A^{-1}$.  $A \in M_n(\R) \subset M_n(\CC)$.  $\overline{A}^T = A^{-1}$ --- унитарный оператор $\CC^n \to \CC^n \implies \exists$ ОНБ из собственных векторов  $e_1, \ldots, e_n$. $V = \omega_1 \oplus \omega_{-1} (\omega_{\lambda_i} \bigoplus\limits_{\lambda_i \neq \pm 1} \omega_{\overline{\lambda}_i})$.

    $\omega_i = \langle e_{i_1}, \ldots, e_{i_k} \rangle, \rk_\CC (A - E) = n - k, \rk_\R (A-E) = n - k \implies \exists$ базис $\widetilde{e_{i_1}}, \ldots, \widetilde{e_{i_k}} \in \R^n$. Ортонормируем его по Граму-Шмидту: $\widetilde{\widetilde{e_{i_k}}}$. Точно так же для  $\omega_{-1}$.

    Все новые вектора ортогональны, так как  $\omega_{1} \perp \omega_{-1}$.  $x_i \neq 1, -1, \omega_{x_i} = \langle e_{j_1}, \ldots , e_{j_k} \rangle$.

    $\begin{pmatrix} a + bi \\ c + di \end{pmatrix} e_{jl} = r_{jl} + is_{jl}$. $A(\overline{e}_{jl}) = \overline{\lambda_i} \overline{e}_{jl} \implies \dim w_{\lambda_i} = \dim w_{\overline{\lambda_i}}$.\\
$\lambda_i = \cos \alpha_i + i\sin \alpha_i$\\
А дальше посмотрите запись, у меня лапки.\\
<<Опять немножечко скомкано получилось, кое-какие детали замяты, но что-то мне надоело \ldots, будем считать, что доказали.>> — Михаил Антипов.
\end{proof}
